{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "work_dir = '/tmp/txl-05-12_23-33-51'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(os.path.join(work_dir, 'model-best.pt'), 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198]\n"
     ]
    }
   ],
   "source": [
    "NL = tokenizer.encode('\\n')\n",
    "print(NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 500])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(NL*4)\n",
    "# Turn into a batch. TODO: take batch_size\n",
    "data.unsqueeze_(1)\n",
    "mems = model.init_mems()\n",
    "\n",
    "def predict(model, data, mems):\n",
    "    tgt_len = data.size(0)\n",
    "    hidden, new_mems = model._forward(data, mems=mems)\n",
    "    pred_hid = hidden[-tgt_len:]\n",
    "    return pred_hid, new_mems\n",
    "\n",
    "pred_hid, mems = predict(model, data, mems)\n",
    "print(pred_hid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 50257]) 0.9999994039535522\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_to_softmax(model, hidden, temperature=1, top_k=0):\n",
    "    \"\"\"Turn a hidden projection into log softmax.\n",
    "    \n",
    "    Adapted from utils/proj_adaptive_softmax.py\n",
    "    \"\"\"\n",
    "    self = model.crit\n",
    "    logits = self._compute_logit(hidden, self.out_layers[0].weight,\n",
    "                                            self.out_layers[0].bias, self.out_projs[0])\n",
    "    logits = top_k_logits(logits, k=top_k)\n",
    "\n",
    "    logits /= temperature\n",
    "    softmax = F.softmax(logits, dim=-1)\n",
    "    return softmax\n",
    "\n",
    "softmax = hidden_to_softmax(model, pred_hid)\n",
    "print(softmax.shape, softmax[0].sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "softmax = hidden_to_softmax(model, pred_hid[-1])\n",
    "print(softmax.shape)\n",
    "prev = torch.multinomial(softmax, num_samples=1)\n",
    "print(prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "## Init\n",
    "data = torch.tensor(NL*4)\n",
    "# Turn into a batch. TODO: take batch_size\n",
    "data.unsqueeze_(1)\n",
    "mems = model.init_mems()\n",
    "output = None\n",
    "\n",
    "## Grab a sample from the last frame, append to result list, append to `data`\n",
    "pred_hid, mems = predict(model, data, mems)\n",
    "softmax = hidden_to_softmax(model, pred_hid[-1])\n",
    "\n",
    "new_sample = torch.multinomial(softmax, num_samples=1).unsqueeze(-1).squeeze(2)\n",
    "data = torch.cat((data, new_sample), dim=0)\n",
    "if not output:\n",
    "    output = new_sample\n",
    "else:\n",
    "    output = torch.cat((output, new_sample), dim=1)\n",
    "print(output.shape, data.shape)\n",
    "## Run through again\n",
    "## Decode results from result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([14, 1])\n",
      " hirefundedatican pending pending pending opio opio lifting filibuster\n"
     ]
    }
   ],
   "source": [
    "## Init\n",
    "data = torch.tensor(NL*4)\n",
    "# Turn into a batch. TODO: take batch_size\n",
    "data.unsqueeze_(1)\n",
    "mems = model.init_mems()\n",
    "output = None\n",
    "\n",
    "length = 10\n",
    "for i in range(length):\n",
    "    ## Grab a sample from the last frame, append to result list, append to `data`\n",
    "    pred_hid, mems = predict(model, data, mems)\n",
    "    softmax = hidden_to_softmax(model, pred_hid[-1])\n",
    "\n",
    "    new_sample = torch.multinomial(softmax, num_samples=1).unsqueeze(-1).squeeze(2)\n",
    "    data = torch.cat((data, new_sample), dim=0)\n",
    "    if output is None:\n",
    "        output = new_sample\n",
    "    else:\n",
    "        output = torch.cat((output, new_sample), dim=0)\n",
    "print(output.shape, data.shape)\n",
    "for i in range(output.size(1)):\n",
    "    print(tokenizer.decode(output[:, i].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a3de4b33215a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/pytorch-pretrained-BERT/pytorch_pretrained_bert/tokenization_gpt2.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pytorch-pretrained-BERT/pytorch_pretrained_bert/tokenization_gpt2.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;34m\"\"\" Tokenize a string. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mbpe_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/regex.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     more than one group. Empty matches are included in the result.\"\"\"\n\u001b[1;32m    332\u001b[0m     return _compile(pattern, flags, kwargs).findall(string, pos, endpos,\n\u001b[0;32m--> 333\u001b[0;31m       overlapped, concurrent)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m def finditer(pattern, string, flags=0, pos=None, endpos=None, overlapped=False,\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
